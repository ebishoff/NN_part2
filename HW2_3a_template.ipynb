{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extract MNIST data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding, reshape = False (that means images are not flatten)\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",reshape=False,one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare training, validation and testing data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "x_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "x_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "#pad images with 0s (28x28 to 32x32)\n",
    "x_train=np.pad(x_train,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "x_validation=np.pad(x_validation, ((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "x_test=np.pad(x_test,((0,0),(2,2),(2,2),(0,0)),'constant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps =10\n",
    "#number of batch_size\n",
    "batch_size = 256\n",
    "\n",
    "#network parameters\n",
    "#hidden_layers=5\n",
    "n_hidden_1_filters = 6\n",
    "n_hidden_2_filters = 16\n",
    "n_hidden_3=120\n",
    "n_hidden_4=84\n",
    "num_input = 32\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 Architecture\n",
    "Input:32 by 32\n",
    "\n",
    "Convolution Layer 1: # of Filters nc=6, Filter size f=5, Padding p=0, Stride, s=1 Thus output is (n^(l-1)+2p-f)/s+1=(32-5)/1+1=28 by 28 by 6\n",
    "\n",
    "Pooling Layer 1: # of filter nc=6, filter size f=2, padding p=0, stride s=2 The output is (28-2)/2+1=14. So 14 by 14 by 6\n",
    "\n",
    "Covolutional Layer 2: # of Filters nc=16, Filter size f=5, Padding p=0, Stride, s=1 Thus output is (n^(l-1)+2p-f)/s+1=(-5)/1+1=28 by 28 by 6. The output is (14-5)/1+1=10. So 10 by 10 by 16\n",
    "\n",
    "Pooling Layer 2: # of filters nc=16, filter size f=2, padding p=0, stride s=2 The output is (10-2)/2+1=5. So 5 by 5 by 16.\n",
    "\n",
    "Fully Connect layer 3\n",
    "We flatten it to be 5x5x16=400 nodes\n",
    "\n",
    "Fully Connected Layer 4\n",
    "It will be 84 nodes\n",
    "\n",
    "Output Layer 5\n",
    "It will be 10 since we have 10 possible values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None,num_input,num_input,1],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#each filter: f x f x nc(l-1)\n",
    "#weights: f x f x nc(l-1) x nc(l)\n",
    "#bias: nc(l)\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1_conv1': tf.Variable(tf.random_normal([5,5,1,6]),name='W1_conv1'),\n",
    "    'W2_conv2': tf.Variable(tf.random_normal([5,5,6,16]),name='W2_conv2'),\n",
    "    'W3': tf.Variable(tf.random_normal([400, n_hidden_3]),name='W3'),\n",
    "    'W4': tf.Variable(tf.random_normal([120, n_hidden_4]),name='W4'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_4, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1_conv1': tf.Variable(tf.zeros(shape=[n_hidden_1_filters]),name='b1_conv1'),\n",
    "    'b2_conv2': tf.Variable(tf.zeros(shape=[n_hidden_2_filters]),name='b2_conv2'),\n",
    "    'b3': tf.Variable(tf.zeros(shape=[n_hidden_3]),name='b3'),\n",
    "    'b4': tf.Variable(tf.zeros(shape=[n_hidden_4]),name='b4'),\n",
    "\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define LeNet-5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def lenet(x):\n",
    "    #k=np.shape(x)[0]\n",
    "    conv_1_out = tf.nn.relu(tf.add(tf.nn.conv2d(x,weights['W1_conv1'],strides=[1,1,1,1],padding='VALID'),biases['b1_conv1']))\n",
    "    pool_1_out=tf.nn.avg_pool(conv_1_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "    conv_2_out = tf.nn.relu(tf.add(tf.nn.conv2d(pool_1_out,weights['W2_conv2'],strides=[1,1,1,1],padding='VALID'),biases['b2_conv2']))\n",
    "    pool_2_out=tf.nn.avg_pool(conv_2_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "    pool_2_flatten=tf.reshape(pool_2_out,[-1,5*5*16])\n",
    "    layer_3_out = tf.nn.relu(tf.add(tf.matmul(pool_2_flatten,weights['W3']),biases['b3']))\n",
    "    layer_4_out = tf.nn.relu(tf.add(tf.matmul(layer_3_out,weights['W4']),biases['b4']))\n",
    "\n",
    "    out = tf.add(tf.matmul(layer_4_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = lenet(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# loss_summary=tf.summary.scalar('loss',loss)\n",
    "# accuracy_summary=tf.summary.scalar('accuracy',accuracy)\n",
    "# #file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training, validating, testing</h1>\n",
    "<h2>1. Print out validation accuracy after each training epoch</h2>\n",
    "<h2>2. Print out training time on each epoch</h2>\n",
    "<h2>3. Print out testing accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Time:101.93583416938782 , Training Accuracy= 0.819\n",
      "Epoch 0, Validation Accuracy= 0.909\n",
      "Epoch 1, Time:111.10178709030151 , Training Accuracy= 0.924\n",
      "Epoch 1, Validation Accuracy= 0.931\n",
      "Epoch 2, Time:96.2876603603363 , Training Accuracy= 0.943\n",
      "Epoch 2, Validation Accuracy= 0.941\n",
      "Epoch 3, Time:107.33812761306763 , Training Accuracy= 0.954\n",
      "Epoch 3, Validation Accuracy= 0.945\n",
      "Epoch 4, Time:106.95116114616394 , Training Accuracy= 0.960\n",
      "Epoch 4, Validation Accuracy= 0.952\n",
      "Epoch 5, Time:107.67722177505493 , Training Accuracy= 0.966\n",
      "Epoch 5, Validation Accuracy= 0.954\n",
      "Epoch 6, Time:107.37203526496887 , Training Accuracy= 0.971\n",
      "Epoch 6, Validation Accuracy= 0.956\n",
      "Epoch 7, Time:76.16144943237305 , Training Accuracy= 0.975\n",
      "Epoch 7, Validation Accuracy= 0.958\n",
      "Epoch 8, Time:68.68642592430115 , Training Accuracy= 0.979\n",
      "Epoch 8, Validation Accuracy= 0.960\n",
      "Epoch 9, Time:68.46801114082336 , Training Accuracy= 0.982\n",
      "Epoch 9, Validation Accuracy= 0.959\n",
      "Training finished!\n",
      "Testing Accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    num=len(x_train)\n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        x_train, y_train=shuffle(x_train, y_train)\n",
    "        acc=[]\n",
    "        st=time.time()\n",
    "        for j in range(0,num,batch_size):\n",
    "            k=j+batch_size\n",
    "            batch_x, batch_y = x_train[j:k], y_train[j:k]\n",
    "        #run optimization\n",
    "            sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "\n",
    "            acc.append(sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y}))\n",
    "        mid=time.time()\n",
    "        print(\"Epoch \"+str(i)+\", Time:{} \".format(mid-st)+\", Training Accuracy= {:.3f}\".format(np.average(acc)))\n",
    "        \n",
    "        val_acc=(sess.run(accuracy,feed_dict={X:x_validation, Y:y_validation}))\n",
    "        print(\"Epoch \"+str(i)+\", Validation Accuracy= {:.3f}\".format(np.average(val_acc)))\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "    tes=sess.run(accuracy, feed_dict={X:x_test, Y:mnist.test.labels})\n",
    "    print(\"Testing Accuracy:\",tes)\n",
    "    #testing_accuracy.append(tes)\n",
    "#file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we were to implement the LeNet-5 model in TensorFlow. With this we were able to achieve a testing accuracy of 95% with a learning rate of .01. We should have been able to get to around 99% accuracy, but since the homework focused on creating the actual LeNet architecture, I didn't worry as much about the testing accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
