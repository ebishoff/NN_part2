{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import load_cifar_template1\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.0001\n",
    "#number of traning steps\n",
    "num_steps =12\n",
    "#number of batch_size\n",
    "batch_size = 256\n",
    "\n",
    "#network parameters\n",
    "#hidden_layers=5\n",
    "n_hidden_1_filters = 6\n",
    "n_hidden_2_filters = 16\n",
    "n_hidden_3=120\n",
    "n_hidden_4=84\n",
    "num_input = 32\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None,num_input,num_input,3],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#each filter: f x f x nc(l-1)\n",
    "#weights: f x f x nc(l-1) x nc(l)\n",
    "#bias: nc(l)\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1_conv1': tf.Variable(tf.random_normal([5,5,3,6]),name='W1_conv1'),\n",
    "    'W2_conv2': tf.Variable(tf.random_normal([5,5,6,16]),name='W2_conv2'),\n",
    "    'W3': tf.Variable(tf.random_normal([400, n_hidden_3]),name='W3'),\n",
    "    'W4': tf.Variable(tf.random_normal([120, n_hidden_4]),name='W4'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_4, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1_conv1': tf.Variable(tf.zeros(shape=[n_hidden_1_filters]),name='b1_conv1'),\n",
    "    'b2_conv2': tf.Variable(tf.zeros(shape=[n_hidden_2_filters]),name='b2_conv2'),\n",
    "    'b3': tf.Variable(tf.zeros(shape=[n_hidden_3]),name='b3'),\n",
    "    'b4': tf.Variable(tf.zeros(shape=[n_hidden_4]),name='b4'),\n",
    "\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LeNet-5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def lenet(x):\n",
    "    conv_1_out = tf.nn.relu(tf.add(tf.nn.conv2d(x,weights['W1_conv1'],strides=[1,1,1,1],padding='VALID'),biases['b1_conv1']))\n",
    "    pool_1_out=tf.nn.avg_pool(conv_1_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "    conv_2_out = tf.nn.relu(tf.add(tf.nn.conv2d(pool_1_out,weights['W2_conv2'],strides=[1,1,1,1],padding='VALID'),biases['b2_conv2']))\n",
    "    pool_2_out=tf.nn.avg_pool(conv_2_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "    pool_2_flatten=tf.reshape(pool_2_out,[-1,5*5*16])\n",
    "    layer_3_out = tf.nn.relu(tf.add(tf.matmul(pool_2_flatten,weights['W3']),biases['b3']))\n",
    "    layer_4_out = tf.nn.relu(tf.add(tf.matmul(layer_3_out,weights['W4']),biases['b4']))\n",
    "\n",
    "    out = tf.add(tf.matmul(layer_4_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = lenet(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# loss_summary=tf.summary.scalar('loss',loss)\n",
    "# accuracy_summary=tf.summary.scalar('accuracy',accuracy)\n",
    "# #file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training, validation and testing</h1>\n",
    "<h2>Train your model only 10 epochs.</h2>\n",
    "<h2>1.Print out validation accuracy after each training epoch</h2>\n",
    "<h2>2.Print out training time for each training epoch</h2>\n",
    "<h2>3.Print out testing accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Time:66.83464860916138 , Accuracy= 0.093\n",
      "Validation Accuracy: 0.1008\n",
      "Epoch 1, Time:66.31645178794861 , Accuracy= 0.079\n",
      "Validation Accuracy: 0.1066\n",
      "Epoch 2, Time:66.30547070503235 , Accuracy= 0.107\n",
      "Validation Accuracy: 0.1082\n",
      "Epoch 3, Time:66.02772092819214 , Accuracy= 0.105\n",
      "Validation Accuracy: 0.114\n",
      "Epoch 4, Time:65.03082919120789 , Accuracy= 0.093\n",
      "Validation Accuracy: 0.11\n",
      "Epoch 5, Time:67.84182262420654 , Accuracy= 0.130\n",
      "Validation Accuracy: 0.1172\n",
      "Epoch 6, Time:64.98388123512268 , Accuracy= 0.133\n",
      "Validation Accuracy: 0.1308\n",
      "Epoch 7, Time:66.76444864273071 , Accuracy= 0.153\n",
      "Validation Accuracy: 0.135\n",
      "Epoch 8, Time:65.55088233947754 , Accuracy= 0.141\n",
      "Validation Accuracy: 0.141\n",
      "Epoch 9, Time:64.08248543739319 , Accuracy= 0.145\n",
      "Validation Accuracy: 0.1508\n",
      "Epoch 10, Time:66.68687057495117 , Accuracy= 0.167\n",
      "Validation Accuracy: 0.1538\n",
      "Epoch 11, Time:66.03376698493958 , Accuracy= 0.153\n",
      "Validation Accuracy: 0.1588\n",
      "Training finished!\n",
      "Testing Accuracy: 0.1535\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        acc=[]\n",
    "        st=time.time()\n",
    "        for j in range(1,6):\n",
    "            for (a,b) in load_cifar_template1.load_preprocessed_training_batch(j,1000):\n",
    "                batch_x, batch_y = load_cifar_template1.features_reshape(a), b\n",
    "                #run optimization\n",
    "                sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "               \n",
    "                acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y})\n",
    "        mid=time.time()\n",
    "        print(\"Epoch \"+str(i)+\", Time:{} \".format(mid-st)+\", Accuracy= {:.3f}\".format(np.average(acc)))\n",
    "        \n",
    "        batch_a,batch_b=load_cifar_template1.load_preprocessed_validation_batch()\n",
    "        batch_a=load_cifar_template1.features_reshape(batch_a)\n",
    "        val=sess.run(accuracy, feed_dict={X:batch_a, Y:batch_b})\n",
    "        print(\"Validation Accuracy:\",val)\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    batch_a,batch_b=load_cifar_template1.load_preprocessed_test_batch()\n",
    "    batch_a=load_cifar_template1.features_reshape(batch_a)\n",
    "    test_acc=sess.run(accuracy, feed_dict={X:batch_a, Y:batch_b})\n",
    "    print(\"Testing Accuracy:\",test_acc)\n",
    "#file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this one, I tweaked the learning rate from .1 to .0001 to get us close to 60% accuracy at around 10 training epochs, but I never could seem to get above 15% accuracy on the testing dataset. Not sure why this is, but since we were more focused on the implementation of the framework I didn't worry about it as much"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
