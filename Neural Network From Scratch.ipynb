{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerual Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did problems 2-4 first and left this one for last. This caused me to run out of time. Thus, I followed a tutorial on creating this neural network from scratch because I wanted to be able to implement it and learn about it, but knew I would not be able to do it on my own in the amount of time I had. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-556ab2150235>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ebish\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=mnist.train.images\n",
    "y_train=mnist.train.labels\n",
    "x_val=mnist.validation.images\n",
    "y_val=mnist.validation.labels\n",
    "x_test=mnist.test.images\n",
    "y_test=mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train)[1])\n",
    "print(np.shape(y_train)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs, Outputs, Layers, Learning rate, cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lay_size(x,y):\n",
    "    input_shape=np.shape(x_train)[1]\n",
    "    num_nodes_hl=300\n",
    "    output_shape=np.shape(y_train)[1]\n",
    "    \n",
    "    return (input_shape, num_nodes_hl,output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(input_shape, num_nodes_h1,output_shape):\n",
    "    w1=np.random.randn(input_shape, num_nodes_h1)*.01\n",
    "    w2=np.random.randn(num_nodes_h1,output_shape)*.01\n",
    "    b1=np.zeros(shape=[num_nodes_h1])\n",
    "    b2=np.zeros(shape=[output_shape])\n",
    "\n",
    "    weights = {\n",
    "        'w1': w1,\n",
    "        'w2': w2   \n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'b1': b1,\n",
    "        'b2': b2\n",
    "    } \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement forward propagation and compute the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x,weights, biases):\n",
    "    w1=weights['w1']\n",
    "    w2=weights['w2']\n",
    "    b1=biases['b1']\n",
    "    b2=biases['b2']\n",
    "    \n",
    "    c1=np.dot(x,w1)+b1\n",
    "    act1= c1*(c1>0)\n",
    "    c2= (np.dot(act1,w2)+b2)\n",
    "    act2= c2*(c2>0)\n",
    "    \n",
    "    new_values={\n",
    "        'c1':c1,\n",
    "        'act1': act1,\n",
    "        'c2': c2,\n",
    "        'act2': act2\n",
    "    }\n",
    "    \n",
    "    return act2, new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(x):\n",
    "#     exps=np.exp(x)\n",
    "#     ans=exps/np.sum(exps)\n",
    "#     return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    p1=np.multiply(np.log(y_hat+.01),y)+np.multiply((1-y),np.log(1-y_hat+.01))\n",
    "    c=-np.sum(p1)/np.shape(y)[0]\n",
    "    c=np.squeeze(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy(y_hat, y):\n",
    "#     p=softmax(x)\n",
    "#     m=np.shape(y)[0]\n",
    "#     log_likelihood=-np.log(p[range(m),y.argmax(axis=1)])\n",
    "#     loss=np.sum(log_likelihood)/m\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the gradients of your cost function and back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_cost_function(y_hat,y):\n",
    "#     m=np.shape(y)[0]\n",
    "#     grad=softmax(x)\n",
    "#     grad[range(m),y.argmax(axis=1)]=grad[range(m),y.argmax(axis=1)]-1\n",
    "#     grad=grad/m\n",
    "#     return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x,y,weights, biases,new_values):\n",
    "    m=np.shape(x)[0]\n",
    "    \n",
    "    w1=weights['w1']\n",
    "    w2=weights['w2']\n",
    "    \n",
    "    act1=new_values['act1']\n",
    "    act2=new_values['act2']\n",
    "    \n",
    "    dc2=act2-y\n",
    "    dw2=(1/m)*np.dot(act1.T,dc2)\n",
    "    db2=(1/m)*np.sum(dc2,axis=0,keepdims=True)\n",
    "    dc1=np.multiply(np.dot(db2,w2.T),1-np.power(act1,2))\n",
    "    dw1=(1/m)*np.dot(x.T,dc1)\n",
    "    db1=(1/m)*np.sum(dc1,axis=0,keepdims=True)\n",
    "    \n",
    "    gradients={\n",
    "        \"dw1\": dw1,\n",
    "        \"dw2\": dw2,\n",
    "        \"db1\": db1,\n",
    "        \"db2\": db2\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(weights, biases, gradients, lr=.01):\n",
    "    w1=weights['w1']\n",
    "    w2=weights['w2']\n",
    "    b1=biases['b1']\n",
    "    b2=biases['b2']\n",
    "    \n",
    "    dw1=gradients['dw1']\n",
    "    dw2=gradients['dw2']\n",
    "    db1=gradients['db1']\n",
    "    db2=gradients['db2']\n",
    "    \n",
    "    #update\n",
    "    w1=w1-lr*dw1\n",
    "    w2=w2-lr*dw2\n",
    "    b1=b1-lr*db1\n",
    "    b2=b2-lr*db2\n",
    "    \n",
    "    weights = {\n",
    "    'w1': w1,\n",
    "    'w2': w2   \n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "    'b1': b1,\n",
    "    'b2': b2\n",
    "    } \n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,y,num_nodes_hl,its):\n",
    "    \n",
    "    input_shape=lay_size(x,y)[0]\n",
    "    output_shape=lay_size(x,y)[2]\n",
    "    \n",
    "    weights, biases = initialize(input_shape, num_nodes_hl, output_shape)\n",
    "    w1 = weights['w1']\n",
    "    w2 = weights['w2']\n",
    "    b1 = biases['b1']\n",
    "    b2 = biases['b2']\n",
    "    \n",
    "    for i in range(0,its):\n",
    "        act2,new_values=forward_prop(x,weights, biases)\n",
    "        \n",
    "        cost=cross_entropy(act2,y)\n",
    "        \n",
    "        gradients=back_prop(x,y,weights, biases,new_values)\n",
    "        \n",
    "        weights, biases=update(weights, biases, gradients, lr=.01)\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"cost after iteration %i: %f\" %(i,cost))\n",
    "        \n",
    "    return weights, biases\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weights, biases, x):\n",
    "    act2,new_values=forward_prop(x,weights, biases)\n",
    "    predictions=act2>.5\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0: 4.309390\n",
      "cost after iteration 100: 2.977251\n",
      "cost after iteration 200: 2.884132\n",
      "cost after iteration 300: 2.797523\n",
      "cost after iteration 400: 2.720751\n"
     ]
    }
   ],
   "source": [
    "weights, biases=model(x_train,y_train,300,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-c8196a5cdba2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = predict(weights, biases, x_train)\n",
    "print ('Accuracy: %d' % float((np.dot(y_train,predictions.T) + np.dot(1-y_train,1-predictions.T))/float(y_train.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-808b35aed1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpredictions_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpredictions_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "predictions_test=predict(weights,biases,x_test)\n",
    "print ('Accuracy: %d' % float((np.dot(y_test,predictions_test.T) + np.dot(1-y_test,1-predictions_test.T))/float(np.shape(y_test)[0])*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow verison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps =500\n",
    "#number of batch_size\n",
    "batch_size = 256\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 300\n",
    "num_input = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(x):\n",
    "    layer_one_out=tf.layers.dense(x,units=300,activation='relu')\n",
    "    out=tf.layers.dense(layer_one_out,units=10)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = nn_model(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0,Training Accuracy= 0.430\n",
      "Cost:  1.8955656\n",
      "step 100,Training Accuracy= 0.953\n",
      "Cost:  0.14390247\n",
      "step 200,Training Accuracy= 0.969\n",
      "Cost:  0.10751048\n",
      "step 300,Training Accuracy= 0.988\n",
      "Cost:  0.064897254\n",
      "step 400,Training Accuracy= 0.984\n",
      "Cost:  0.053828616\n",
      "Training finished!\n",
      "Testing Accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        if i % 100 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y})\n",
    "            los=sess.run(loss,feed_dict={X:batch_x, Y:batch_y})\n",
    "            print(\"step \"+str(i)+\",Training Accuracy= {:.3f}\".format(acc))\n",
    "            print('Cost: ',los)\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran out of time since this was the last thing I focused on out of all the questions so I did not finish everything for this one, but still found some valuable insight. \n",
    "\n",
    "The Neural Network from scratch took a lot longer to train. The tensorflow verison seemed to be 30 times faster. This is probably because they have coded tensorflow to be as efficient as possible where my code is to just get results. The cost is lower for the tensorflow, but not by as much as I would have thought.\n",
    "\n",
    "The accuracy for for tensorflow on the testing accuracy is 97% which is pretty good. I couldn't get my accuracy for the numpy NN to work so I can't seem to compare that. Also, I didn't implement the Adam Optimier so I can't compare between those either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
